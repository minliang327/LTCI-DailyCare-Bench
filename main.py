"""
Green Agent Benchmark ä¸»ç¨‹åº (ä¿®å¤ç‰ˆ + æ ‡å‡†ç­”æ¡ˆè§£æ)
"""
import json
import argparse
from typing import Dict, Any, List
from models import AssessmentInput, DailyPlan, ScoreResult
from evaluator import GreenAgentEvaluator
from generator import BaselineGenerator
from database import get_required_tasks, get_task_info, ASSESSMENT_RULES

# å°è¯•å¯¼å…¥åç”Ÿæˆå™¨
try:
    from bad_generator import generate_bad_plan
    HAS_BAD_GENERATOR = True
except ImportError:
    HAS_BAD_GENERATOR = False

def load_assessment_from_json(file_path: str) -> AssessmentInput:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return AssessmentInput(**data)

def load_plan_from_json(file_path: str) -> DailyPlan:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return DailyPlan(**data)

def create_sample_assessment() -> AssessmentInput:
    return AssessmentInput(
        assessment_id="ASSESS_001",
        patient_info={"name": "å¼ å…ˆç”Ÿ", "age": 75, "gender": "ç”·"},
        assessment_data={
            "é¥®é£Ÿä¹ æƒ¯": "ä½ç³–æˆ–æ— ç³–",
            "è¡£ç€æ•´æ´": 3,
            "è¿‡æ•æƒ…å†µ": "é£Ÿç‰©è¿‡æ•",
            "è·Œå€’é£é™©": True,
            "è¡ŒåŠ¨èƒ½åŠ›": "éƒ¨åˆ†ä¸èƒ½",
            "å¦‚å•èƒ½åŠ›": "éƒ¨åˆ†ä¸èƒ½",
            "æ´—æµ´èƒ½åŠ›": "å®Œå…¨ä¸èƒ½",
            "ç•™ç½®å°¿ç®¡": "å¦",
            "éœ€è¦ç›‘æµ‹è¡€ç³–": True
        },
        eating_habits="ä½ç³–æˆ–æ— ç³–",
        clothing_neatness=3,
        allergy_info="é£Ÿç‰©è¿‡æ•",
        fall_risk=True
    )

def print_ground_truth(assessment: AssessmentInput):
    """ã€æ–°åŠŸèƒ½ã€‘æ‰“å°æ ‡å‡†ç­”æ¡ˆè§£æï¼šæ˜¾ç¤ºè¯„ä¼°å•è§¦å‘äº†å“ªäº›è§„åˆ™"""
    print("\n" + "="*60)
    print("ğŸ” æ ‡å‡†ç­”æ¡ˆè§£æ (Ground Truth Analysis)")
    print("="*60)
    print("æ ¹æ®è¯„ä¼°å•æ•°æ®ï¼ŒGreen Agent æ¨å¯¼å‡ºçš„ã€å¿…é¡»æ‰§è¡Œä»»åŠ¡ã€‘å¦‚ä¸‹ï¼š")
    
    # 1. éå†è¯„ä¼°å•ä¸­çš„æ‰€æœ‰é”®å€¼å¯¹
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œç›´æ¥ç”¨ assessment_data æ¥åŒ¹é… database é‡Œçš„è§„åˆ™
    triggered_rules = []
    
    # æ£€æŸ¥ assessment_data é‡Œçš„æ¯ä¸€é¡¹
    for key, value in assessment.assessment_data.items():
        # æ„é€ å¯èƒ½çš„æŸ¥è¯¢æ¡ä»¶ï¼Œä¾‹å¦‚ "è·Œå€’é£é™©" æˆ– "é¥®é£Ÿä¹ æƒ¯: ä½ç³–"
        # ç®€å•é€»è¾‘ï¼šæ£€æŸ¥ key æ˜¯å¦åœ¨è§„åˆ™åº“ï¼Œæˆ–è€… "key: value" æ˜¯å¦åœ¨è§„åˆ™åº“
        
        # å°è¯•åŒ¹é… "Key: Value" æ ¼å¼ (ä¾‹å¦‚ "é¥®é£Ÿä¹ æƒ¯: ä½ç³–æˆ–æ— ç³–")
        condition_str = f"{key}: {value}"
        required_ids = get_required_tasks(condition_str)
        
        # å¦‚æœæ²¡åŒ¹é…åˆ°ï¼Œå°è¯•åŒ¹é… Key (ä¾‹å¦‚ "è·Œå€’é£é™©" ä¸º True æ—¶)
        if not required_ids and value is True:
            required_ids = get_required_tasks(key)
            
        if required_ids:
            task_names = []
            for tid in required_ids:
                info = get_task_info(tid)
                task_names.append(f"[{tid}]{info.get('name', 'æœªçŸ¥')}")
            
            print(f"  â€¢ æ£€æµ‹åˆ° '{key}: {value}'")
            print(f"    -> è§¦å‘è§„åˆ™ï¼Œè¦æ±‚ä»»åŠ¡: {', '.join(task_names)}")

    print("="*60 + "\n")

def print_result(result: ScoreResult, title="è¯„ä¼°ç»“æœ"):
    print("\n" + "-"*60)
    print(title)
    print("-"*60)
    print(f"æ€»åˆ†: {result.overall_score:.3f} ({'é€šè¿‡' if result.passed else 'æœªé€šè¿‡'})")
    print(f"æ˜ç»†: è¦†ç›–ç‡ {result.breakdown.mandatory_coverage:.0%} | å®‰å…¨ {result.breakdown.safety_score} | èµ„è´¨ {result.breakdown.qualification_score}")
    
    if result.breakdown.mandatory_missing:
        print(f"\n[!] ç¼ºå¤±ä»»åŠ¡: {result.breakdown.mandatory_missing}")
    if result.breakdown.safety_violations:
        print(f"\n[!] å®‰å…¨è¿è§„: {result.breakdown.safety_violations}")
    if result.breakdown.qualification_issues:
        print(f"\n[!] èµ„è´¨è¿è§„:")
        for issue in result.breakdown.qualification_issues:
            print(f"    ä»»åŠ¡{issue['task_id']} éœ€è¦ {issue['required']}, å®é™… {issue['assigned']}")
    print("-"*60 + "\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["demo", "evaluate", "generate"], default="demo")
    parser.add_argument("--assessment", type=str)
    parser.add_argument("--plan", type=str)
    parser.add_argument("--output", type=str)
    args = parser.parse_args()
    
    if args.mode == "demo":
        print("Green Agent Benchmark - æ¼”ç¤ºæ¨¡å¼\n")
        assessment = create_sample_assessment()
        
        # ã€æ–°å¢ã€‘æ‰“å°æ ‡å‡†ç­”æ¡ˆè§£æï¼Œæ»¡è¶³ä½ çš„éœ€æ±‚
        print_ground_truth(assessment)
        
        # 1. Good Agent
        print("ğŸ¤– æµ‹è¯• 1: åŸºå‡†ç”Ÿæˆå™¨ (Good Agent)...")
        generator = BaselineGenerator(target_duration=120)
        plan = generator.generate_perfect_plan(assessment)
        evaluator = GreenAgentEvaluator()
        result = evaluator.evaluate(assessment, plan)
        print_result(result, title="âœ… Good Agent ç»“æœ")

        # 2. Bad Agent
        if HAS_BAD_GENERATOR:
            print("ğŸ¤– æµ‹è¯• 2: å¯¹æŠ—æ€§æµ‹è¯• (Bad Agent)...")
            bad_plan = generate_bad_plan(assessment)
            bad_score = evaluator.evaluate(assessment, bad_plan)
            print_result(bad_score, title="âŒ Bad Agent ç»“æœ (æˆåŠŸæ‹¦æˆª)")
    
    elif args.mode == "evaluate":
        assessment = load_assessment_from_json(args.assessment)
        plan = load_plan_from_json(args.plan)
        # è¯„ä¼°æ¨¡å¼ä¹Ÿæ‰“å°è§£æ
        print_ground_truth(assessment)
        evaluator = GreenAgentEvaluator()
        result = evaluator.evaluate(assessment, plan)
        print_result(result)
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)

    elif args.mode == "generate":
        assessment = load_assessment_from_json(args.assessment)
        generator = BaselineGenerator(target_duration=120)
        plan = generator.generate_perfect_plan(assessment)
        print(json.dumps(plan.model_dump(), ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()